{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main364.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeongjibsa/PTH-Picture-To-Hangeul--OCR/blob/master/main364.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXppY8nLxAvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # OS\n",
        "# !cat /etc/issue.net\n",
        "# # CPU 사양\n",
        "# !head /proc/cpuinfo\n",
        "# # 메모리 사양\n",
        "# !head -n 3 /proc/meminfo\n",
        "# # 디스크 사양\n",
        "# !df -h\n",
        "# # 파이썬 버전\n",
        "# !python --version\n",
        "# # GPU 사양\n",
        "# !nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEk_wE85JXCf",
        "colab_type": "code",
        "outputId": "9c455a1a-59f1-4a10-c56d-3bc3bb912fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.5)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgt08k2UyWl0",
        "colab_type": "code",
        "outputId": "0fd5118b-cc48-4377-80a7-8701b0e5c3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2dYe-VIyt33",
        "colab_type": "code",
        "outputId": "d485ae92-2627-4470-a347-ddfc3824ed19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls '/content/drive/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tFtO_z1dbRI",
        "colab_type": "code",
        "outputId": "38a442b1-6970-4baf-d614-df17bab210e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import os\n",
        "import cv2\n",
        "import re\n",
        "import tempfile\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "from unicodedata import normalize\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tflearn.layers.conv import global_avg_pool\n",
        "\n",
        "print(tf.__version__)\n",
        "print (os.getcwd())\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "csv_path = '/content/drive/My Drive/data/save364.csv'\n",
        "tcsv_path = '/content/drive/My Drive/data/tsave364.csv'\n",
        "train_data_folder = '/content/drive/My Drive/data/image-data/trainset'\n",
        "test_data_folder = '/content/drive/My Drive/data/image-data/testset'\n",
        "train_data_pickle = '/content/drive/My Drive/data/train364.pickle'\n",
        "test_data_pickle = '/content/drive/My Drive/data/test364.pickle'\n",
        "model_path = '/content/drive/My Drive/data/model364/model.ckpt'\n",
        "\n",
        "text = \"가각간감강같개거건검것게격결경계고공과관광괴교구국군권귄규그극근금급기긴길김깊까꽃끼낄나난날남납내너네년노농뉴느는늘능니다단달\" \\\n",
        "       \"담당대더던덜데도독동되된될두드득든들등디따때떠또라란람래량러런럽레려력련령로록론료루류르른를름리린릴립마만많말망매며면명모목못\" \\\n",
        "       \"무문물미민및바박반받발방배백버번범법변별병보복본부북분불비쁘사산살삶삼상새색생서석선설성세셀소속송수순술스슴습시식신실싫심아안\" \\\n",
        "       \"않알았야약양어억언업없었에엑여역연열였영예오온올와완왜외요용우욱운울웃원월위유육윤율으은을음응의이인일읽임입있자작잖잘장재쟁저\" \\\n",
        "       \"적전절점접정제져조족존졸종주준줄중증지직진질집짝차찬참책처천철청체초총최추축출충측치컴크큼키타태터템토통투트특팀파판퍼펭편평포\" \\\n",
        "       \"표품퓨프픈피필하학한할함합항해했행향험헬혁현협형호혹화확환활회효후훈히걷겪께넘녀랑맨뽑승싶워잠줘팅\"\n",
        "integer_encoding = dict(enumerate(text))\n",
        "char_encoding = {v: k for k, v in integer_encoding.items()}\n",
        "\n",
        "training_epochs = 40  # 전체 데이터셋을 train하는 횟수\n",
        "num_models = 1  # 모델 개수\n",
        "batch_size = 128  # 메모리에 올리는 batch data의 개수\n",
        "y_size = len(text) # y클래스 개수\n",
        "start_learning_rate = 1e-6\n",
        "img_width = 64 \n",
        "img_height = 64\n",
        "crop_width = 16\n",
        "crop_height = 16"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "1.14.0\n",
            "/content\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKNZxOnEdjyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(datasetFolder=train_data_folder, testsetFolder=test_data_folder, maxAug=150):\n",
        "    data = {\n",
        "        'path': [],\n",
        "        'label': []\n",
        "    }\n",
        "    tdata = {\n",
        "        'path': [],\n",
        "        'label': []\n",
        "    }\n",
        "    for img in os.listdir(datasetFolder):\n",
        "        img = normalize('NFC', img)\n",
        "        if os.path.isfile(datasetFolder + \"/\" + img):\n",
        "            data['path'].append(datasetFolder + \"/\" + img)\n",
        "            data['label'].append(img[0:1])\n",
        "\n",
        "    for img in os.listdir(testsetFolder):\n",
        "        img = normalize('NFC', img)\n",
        "        if os.path.isfile(testsetFolder + \"/\" + img):\n",
        "            tdata['path'].append(testsetFolder + \"/\" + img)\n",
        "            tdata['label'].append(img[0:1])\n",
        "\n",
        "    \n",
        "    return data, tdata\n",
        "  \n",
        "class Csv:\n",
        "    def __init__(self, data=None, tdata=None):\n",
        "        self.data = {\n",
        "            'path': [],\n",
        "            'label': []\n",
        "        }\n",
        "        self.tdata = {\n",
        "            'path': [],\n",
        "            'label': []\n",
        "        }\n",
        "        if (data == None) and (tdata == None):\n",
        "          return\n",
        "        else:\n",
        "          self.data['path'] = data['path']\n",
        "          self.data['label'] = data['label']\n",
        "          self.tdata['path'] = tdata['path']\n",
        "          self.tdata['label'] = tdata['label']\n",
        "\n",
        "\n",
        "    def saveCsv(self, csvPath=csv_path, tcsvPath=tcsv_path):\n",
        "      try:\n",
        "        df_train = pd.DataFrame(self.data)\n",
        "        df_train.to_csv(csvPath, encoding='euc-kr')\n",
        "      except:\n",
        "        print(\"save csv error\")\n",
        "      try:\n",
        "        df_test = pd.DataFrame(self.tdata)\n",
        "        df_test.to_csv(tcsvPath, encoding='euc-kr')\n",
        "      except:\n",
        "        print(\"save tcsv error\")\n",
        "      \n",
        "    def loadCsv(self, csvPath=csv_path, tcsvPath=tcsv_path):\n",
        "      try:\n",
        "        df_train = pd.read_csv(csvPath, sep=\",\", engine='python')\n",
        "        df_test = pd.read_csv(tcsvPath, sep=\",\", engine='python')\n",
        "      except:\n",
        "        print(\"load csv, tcsv error\")\n",
        "      return df_train, df_test\n",
        "    \n",
        "#data, tdata = prepareData()\n",
        "#sf = Csv(data, tdata)\n",
        "sf = Csv()\n",
        "#sf.saveCsv()\n",
        "data, tdata = sf.loadCsv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owCCiYMZuaPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LoadImage:\n",
        "  def __init__(self, data, tdata):\n",
        "    self.data = {\n",
        "            'path': [],\n",
        "            'label': []\n",
        "    }\n",
        "    self.tdata = {\n",
        "            'path': [],\n",
        "            'label': []\n",
        "    }\n",
        "    self.trainData = {\n",
        "        'img': [],\n",
        "        'label': []\n",
        "    }\n",
        "    self.testData = {\n",
        "        'img': [],\n",
        "        'label': []\n",
        "    }\n",
        "    \n",
        "    self.data['path'] = data.path.tolist()\n",
        "    self.data['label'] = data.label.tolist()\n",
        "    self.tdata['path'] = tdata.path.tolist()\n",
        "    self.tdata['label'] = tdata.label.tolist()\n",
        "    \n",
        "    self.trainData['label'] = data.label.tolist()\n",
        "    self.testData['label'] = tdata.label.tolist()\n",
        "    \n",
        "  def imreadEX(self, image_path):\n",
        "    if re.compile('[^ㄱ-ㅣ가-힣]+').sub('', image_path):\n",
        "        stream = open(image_path, \"rb\")\n",
        "        bytes = bytearray(stream.read())\n",
        "        numpyarray = np.asarray(bytes, dtype=np.uint8)\n",
        "        img = cv2.imdecode(numpyarray, cv2.IMREAD_COLOR)\n",
        "        if not img is None:\n",
        "            return img\n",
        "        else:\n",
        "            file_tmp = tempfile.NamedTemporaryFile().name\n",
        "            shutil.copy(image_path, file_tmp)\n",
        "            image_path = file_tmp\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    return img\n",
        "\n",
        "  def loadImg(self):\n",
        "    for i, _ in enumerate(data['path']):\n",
        "      readImg = self.imreadEX(data['path'][i])\n",
        "      readImg = cv2.cvtColor(readImg, cv2.COLOR_BGR2GRAY)\n",
        "      readImg = cv2.resize(readImg, (img_width+crop_width, img_height+crop_height), interpolation=cv2.INTER_AREA)\n",
        "      \n",
        "      #random crop\n",
        "      x = np.random.randint(0, crop_width)\n",
        "      y = np.random.randint(0, crop_height)\n",
        "      readImg = readImg[x: x+img_width, y: y+img_height]\n",
        "      \n",
        "            \n",
        "      readImg = np.reshape(readImg, img_width * img_height).astype(float)\n",
        "      readImg /= np.max(readImg)\n",
        "      self.trainData['img'].append(readImg)\n",
        "      \n",
        "      print('\\rtrainData loading... %d' % (i/len(data['path'])*100) + '%', end='')\n",
        "      \n",
        "      \n",
        "    for i, _ in enumerate(tdata['path']):\n",
        "      readImg = self.imreadEX(tdata['path'][i])\n",
        "      readImg = cv2.cvtColor(readImg, cv2.COLOR_BGR2GRAY)\n",
        "      readImg = cv2.resize(readImg, (img_width, img_height), interpolation=cv2.INTER_AREA)\n",
        "      readImg = np.reshape(readImg, img_width * img_height).astype(float)\n",
        "      readImg /= np.max(readImg)\n",
        "      self.testData['img'].append(readImg)\n",
        "      \n",
        "      print('\\rtestData loading... %d' % (i/len(tdata['path'])*100) + '%', end='')\n",
        "    \n",
        "\n",
        "    return self.trainData, self.testData\n",
        "    \n",
        "    def saveImgToPickle(self, trainData, testData):\n",
        "        f = open('./train.pickle', 'wb')\n",
        "        pickle.dump(trainData, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()\n",
        "        f = open('./test.pickle', 'wb')\n",
        "        pickle.dump(testData, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()  \n",
        "\n",
        "load = LoadImage(data, tdata)\n",
        "#data, tdata = load.loadImg()\n",
        "#load.saveImgToPickle(data, tdata)\n",
        "\n",
        "f = open(train_data_pickle, 'rb')\n",
        "data = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(test_data_pickle, 'rb')\n",
        "tdata = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOMUmDTAbjel",
        "colab_type": "code",
        "outputId": "bf12ae12-2f83-4f04-ef0e-3e908d90bcf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.name_scope(self.name):\n",
        "            self.X = tf.placeholder(tf.float32, [None, img_width * img_height], name='X')\n",
        "            # img 28X28X1 (black/white)\n",
        "            X_img = tf.reshape(self.X, [-1, img_width, img_height, 1])\n",
        "\n",
        "            self.Y = tf.placeholder(tf.float32, [None, y_size], name='Y')\n",
        "            self.training = tf.placeholder(tf.bool, name='training')\n",
        "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "            learning_rate = tf.train.exponential_decay(start_learning_rate, global_step,\n",
        "                                                       training_epochs, 0.99, staircase=True)\n",
        "            \n",
        "            \n",
        "\n",
        "            \n",
        "            # Convolutional Layer #1\n",
        "            conv1 = tf.layers.conv2d(inputs=X_img, filters=64, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            # Pooling Layer #1\n",
        "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
        "                                         rate=0.3, training=self.training)\n",
        "            print(\"l1\", dropout1) # 32x32x32\n",
        "            \n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            shortcut = tf.layers.conv2d(inputs=dropout1, filters=128, kernel_size=[1, 1], strides=(1, 1),\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=128, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv2 = shortcut + conv2\n",
        "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
        "                                         rate=0.3, training=self.training)\n",
        "            print(\"l2\", dropout2)\n",
        "\n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            shortcut = tf.layers.conv2d(inputs=dropout2, filters=256, kernel_size=[1, 1], strides=(1, 1),\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=256, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv3 = shortcut + conv3\n",
        "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
        "                                         rate=0.3, training=self.training)\n",
        "            print(\"l3\", dropout3)\n",
        "\n",
        "            # Convolutional Layer #4 and Pooling Layer #4\n",
        "            shortcut = tf.layers.conv2d(inputs=dropout3, filters=512, kernel_size=[1, 1], strides=(1, 1),\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv4 = tf.layers.conv2d(inputs=dropout3, filters=512, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv4 = shortcut + conv4\n",
        "            pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout4 = tf.layers.dropout(inputs=pool4,\n",
        "                                         rate=0.3, training=self.training)\n",
        "            print(\"l4\", dropout4)\n",
        "\n",
        "            # Convolutional Layer #5 and Pooling Layer #5\n",
        "            shortcut = tf.layers.conv2d(inputs=dropout4, filters=1024, kernel_size=[1, 1], strides=(1, 1),\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv5 = tf.layers.conv2d(inputs=dropout4, filters=1024, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            conv5 = shortcut + conv5\n",
        "            pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout5 = tf.layers.dropout(inputs=pool5,\n",
        "                                         rate=0.3, training=self.training)\n",
        "            print(\"l5\", dropout5)\n",
        "\n",
        "            \n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(dropout5, [-1, 2 * 2 * 1024])\n",
        "            print(\"flat\", flat)\n",
        "            \n",
        "            dense1 = tf.layers.dense(inputs=flat,\n",
        "                                    units=10000, activation=tf.nn.relu)\n",
        "            print(\"dense1\", dense1)\n",
        "            dense2 = tf.layers.dense(inputs=dense1,\n",
        "                                    units=10000, activation=tf.nn.relu)\n",
        "            print(\"dense2\", dense2)\n",
        "            \n",
        "            dropout6 = tf.layers.dropout(inputs=dense2,\n",
        "                                         rate=0.5, training=self.training)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> y_size outputs\n",
        "            self.logits = tf.layers.dense(inputs=dropout6, units=y_size, name='logits')\n",
        "            print(\"logits\", self.logits)\n",
        "        \n",
        "        with tf.name_scope(self.name + '-cost'):\n",
        "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.Y))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost,\n",
        "                                                                                          global_step=global_step)\n",
        "\n",
        "        with tf.name_scope(self.name + '-eval'):\n",
        "            correct_prediction = tf.equal(tf.math.argmax(self.logits, 1), tf.math.argmax(self.Y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.training: training})\n",
        "        \n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer],\n",
        "                             feed_dict={self.X: x_data, self.Y: y_data, self.training: training})\n",
        "\n",
        "    def result(self, x_test, training=False):\n",
        "        label = tf.math.argmax(self.logits, 1)\n",
        "        label = self.sess.run(label, feed_dict={self.X: x_test, self.training: training})\n",
        "        return list(label)\n",
        "\n",
        "\n",
        "def dataTrain(trainData, testData):\n",
        "    if not trainData:\n",
        "        print(\"없는데?\")\n",
        "        return\n",
        "    print(\"있는데?\")\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    models = []\n",
        "    \n",
        "    for m in range(num_models):\n",
        "        models.append(Model(sess, \"model\" + str(m)))\n",
        "\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "\n",
        "    batch_xs, batch_ys, tbatch_xs, tbatch_ys = trans_label(trainData, testData,\n",
        "                                                           sess)  # label 숫자로 변환 -> onehot encoding return.\n",
        "\n",
        "    if tf.train.checkpoint_exists(model_path):\n",
        "        print('load success')\n",
        "        saver.restore(sess, model_path)\n",
        "    else:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    print('Learning Started')\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost_list = np.zeros(len(models))\n",
        "        total_batch = int(len(batch_ys) / batch_size + 1)\n",
        "        batch_xs, batch_ys = data_shuffle(batch_xs, batch_ys)\n",
        "        for i in range(total_batch):\n",
        "            start = i * batch_size\n",
        "            end = start + batch_size\n",
        "            mini_xs, mini_ys = batch_xs[start:end], batch_ys[start:end]\n",
        "            for m_index, m in enumerate(models):\n",
        "                c, _ = m.train(mini_xs, mini_ys)\n",
        "                avg_cost_list[m_index] += c / total_batch\n",
        "        \n",
        "        print('\\rEpoch: ', '%04d' % (epoch + 1), 'cost = ', avg_cost_list)\n",
        "        \n",
        "        if ((epoch+1) % 20) == 0:\n",
        "          saver.save(sess, model_path)\n",
        "          print(\"model save complete\")\n",
        "        \n",
        "    saver.save(sess, model_path) #모델저장\n",
        "    print('Learning Finished')\n",
        "\n",
        "    test_size = len(tbatch_ys)\n",
        "    predictions = np.zeros([0, y_size])\n",
        "    mini_predictions = np.zeros([batch_size, y_size])\n",
        "    \n",
        "    total_batch = int(test_size / batch_size + 1)\n",
        "    for i in range(total_batch):\n",
        "      start = i * batch_size\n",
        "      end = start + batch_size\n",
        "      mini_xs, mini_ys = tbatch_xs[start:end], tbatch_ys[start:end]\n",
        "      for m_index, m in enumerate(models):\n",
        "        print(m_index, \"Accuracy: \", m.get_accuracy(mini_xs, mini_ys))\n",
        "        p = m.predict(mini_xs)\n",
        "        mini_predictions = p\n",
        "      predictions = np.vstack((predictions, mini_predictions))\n",
        "      \n",
        "    ensemble_correct_prediction = tf.equal(\n",
        "        tf.argmax(predictions, 1), tf.argmax(tbatch_ys, 1))\n",
        "    ensemble_accuracy = tf.reduce_mean(\n",
        "        tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "    print('Ensemble accuracy:', sess.run(ensemble_accuracy))\n",
        "    \n",
        "\n",
        "\n",
        "def trans_label(trainData, testData, sess):\n",
        "    batch_xs = trainData['img']\n",
        "    batch_ys = trainData['label']\n",
        "    \n",
        "    for i, _ in enumerate(batch_ys):\n",
        "        batch_ys[i] = char_encoding[batch_ys[i]]\n",
        "      \n",
        "    batch_ys = tf.one_hot(batch_ys, depth=y_size).eval(session=sess)\n",
        "    batch_ys = np.reshape(batch_ys, [-1, y_size])\n",
        "\n",
        "    tbatch_xs = testData['img']\n",
        "    tbatch_ys = testData['label']\n",
        "\n",
        "    for i, _ in enumerate(tbatch_ys):\n",
        "        tbatch_ys[i] = char_encoding[tbatch_ys[i]]\n",
        "\n",
        "    tbatch_ys = tf.one_hot(tbatch_ys, depth=y_size).eval(session=sess)\n",
        "    tbatch_ys = np.reshape(tbatch_ys, [-1, y_size])\n",
        "    \n",
        "    return batch_xs, batch_ys, tbatch_xs, tbatch_ys\n",
        "\n",
        "\n",
        "def data_shuffle(data_x, data_y):\n",
        "    tmp = [[x, y] for x, y in zip(data_x, data_y)]\n",
        "    random.shuffle(tmp)\n",
        "    x = [n[0] for n in tmp]\n",
        "    y = [n[1] for n in tmp]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def decode_label(label):\n",
        "    for i in range(len(label)):\n",
        "        label[i] = integer_encoding[label[i]]\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "dataTrain(data, tdata)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "있는데?\n",
            "WARNING:tensorflow:From <ipython-input-8-8ad95c6beac6>:24: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-8-8ad95c6beac6>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-8ad95c6beac6>:29: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "l1 Tensor(\"model0/dropout/cond/Merge:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
            "l2 Tensor(\"model0/dropout_1/cond/Merge:0\", shape=(?, 16, 16, 128), dtype=float32)\n",
            "l3 Tensor(\"model0/dropout_2/cond/Merge:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
            "l4 Tensor(\"model0/dropout_3/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
            "l5 Tensor(\"model0/dropout_4/cond/Merge:0\", shape=(?, 2, 2, 1024), dtype=float32)\n",
            "flat Tensor(\"model0/Reshape_1:0\", shape=(?, 4096), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-8-8ad95c6beac6>:86: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "dense1 Tensor(\"model0/dense/Relu:0\", shape=(?, 10000), dtype=float32)\n",
            "dense2 Tensor(\"model0/dense_1/Relu:0\", shape=(?, 10000), dtype=float32)\n",
            "logits Tensor(\"model0/logits/BiasAdd:0\", shape=(?, 364), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-8-8ad95c6beac6>:143: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "load success\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/data/model364/model.ckpt\n",
            "Learning Started\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}